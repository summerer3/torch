{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        初始化数据集。\n",
    "        :param root_dir: 包含图像文件的根目录。\n",
    "        :param transform: 应用于图像的可选变换。\n",
    "        \"\"\"\n",
    "        self.root_dir = os.path.join(root_dir, '.json')\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        # 遍历目录，收集图像路径和标签\n",
    "        for filename in np.sort(os.listdir(root_dir)):\n",
    "            if filename.endswith('.json'):  # 假设图像文件后缀为.jpg\n",
    "                img_path = os.path.join('/media/liushilei/DatAset/workspace/test/torch/data/nyc/cut_data', os.path.basename(filename).split('.')[0] + '.png')\n",
    "                filename = os.path.join('/media/liushilei/DatAset/workspace/test/torch/data/labels/annotation_seq', filename)\n",
    "                self.images.append(img_path)\n",
    "                self.labels.append(filename)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        返回数据集中的图像数量。\n",
    "        \"\"\"\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        根据索引获取一个图像和它的标签。\n",
    "        \"\"\"\n",
    "        image_path = self.images[idx]\n",
    "        image = Image.open(image_path).convert('RGB')  # 确保图像是RGB格式\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        with open(label, 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        # 遍历JSON中的每个元素\n",
    "        label = torch.zeros(100, 500, 2)\n",
    "        for i, item in enumerate(data):\n",
    "            seq = item.get(\"seq\", [])\n",
    "            n = len(seq)\n",
    "            if n > 1:\n",
    "                label[i, :n, :] = torch.tensor(seq)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# 创建数据集的变换\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((256, 256)),  # 调整图像大小\n",
    "    transforms.ToTensor(),  # 转换为Tensor\n",
    "])\n",
    "\n",
    "# 创建数据集实例\n",
    "dataset = CustomDataset(root_dir='data/labels/annotation_seq', transform=transform)\n",
    "\n",
    "# 现在可以使用PyTorch的DataLoader来加载数据集\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "H, W = 1000, 1000\n",
    "class RoadEdgeDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RoadEdgeDetector, self).__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.rnn = nn.LSTM(input_size=256 * (H // 8) * (W // 8), hidden_size=32, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(256, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        features = self.backbone(x)\n",
    "        features = features.view(batch_size, -1)\n",
    "        features = features.unsqueeze(1)\n",
    "        out, _ = self.rnn(features)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RoadEdgeDetector().to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可视化地图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json, os\n",
    "# from PIL import Image, ImageDraw\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def draw_lines_on_image(image_path, json_path):\n",
    "#     # 读取图像\n",
    "#     image = Image.open(image_path)\n",
    "#     draw = ImageDraw.Draw(image)\n",
    "\n",
    "#     # 读取JSON文件\n",
    "#     with open(json_path, 'r') as file:\n",
    "#         data = json.load(file)\n",
    "\n",
    "#     # 遍历JSON中的每个元素\n",
    "#     for item in data:\n",
    "#         seq = item.get(\"seq\", [])\n",
    "#         # 将seq中的点连成线\n",
    "#         if len(seq) > 1:\n",
    "#             for i in range(len(seq) - 1):\n",
    "#                 start_point = tuple(seq[i])\n",
    "#                 end_point = tuple(seq[i + 1])\n",
    "#                 draw.line([start_point, end_point], fill='red', width=3)\n",
    "\n",
    "#     # 可视化图像\n",
    "#     plt.imshow(image)\n",
    "#     plt.axis('off')  # 不显示坐标轴\n",
    "#     plt.show()\n",
    "\n",
    "# # 示例用法\n",
    "# json_path = '/media/liushilei/DatAset/workspace/test/torch/data/labels/annotation_seq/002232_34.json'  # 替换为你的JSON文件路径\n",
    "# image_path = os.path.join('/media/liushilei/DatAset/workspace/test/torch/data/nyc/cut_data', os.path.basename(json_path).split('.')[0] + '.png')  # 替换为你的图像路径\n",
    "\n",
    "# draw_lines_on_image(image_path, json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "H, W = 1000, 1000\n",
    "class RoadEdgeDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RoadEdgeDetector, self).__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.rnn = nn.LSTM(input_size=32 * (H // 8) * (W // 8), hidden_size=32, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        features = self.backbone(x)\n",
    "        features = features.view(batch_size, -1)\n",
    "        features = features.unsqueeze(1)\n",
    "        out, _ = self.rnn(features)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RoadEdgeDetector().to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3003536/4260261636.py:5: UserWarning: Using a target size (torch.Size([8, 100, 500, 2])) that is different to the input size (torch.Size([8, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(pred, target)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (100) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m---> 19\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mpolyline_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m, in \u001b[0;36mpolyline_loss\u001b[0;34m(pred, target)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpolyline_loss\u001b[39m(pred, target):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# 示例损失函数，可以根据实际需求调整\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/maptr/lib/python3.8/site-packages/torch/nn/functional.py:3089\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3086\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3087\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3089\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3090\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m~/anaconda3/envs/maptr/lib/python3.8/site-packages/torch/functional.py:73\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (100) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def polyline_loss(pred, target):\n",
    "    # 示例损失函数，可以根据实际需求调整\n",
    "    loss = F.mse_loss(pred, target)\n",
    "    return loss\n",
    "\n",
    "num_epochs = 50\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in data_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = polyline_loss(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maptr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
