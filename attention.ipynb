{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\" Scaled Dot-Product Attention \"\"\"\n",
    "\n",
    "    def __init__(self, scale):\n",
    "        super().__init__()\n",
    "\n",
    "        self.scale = scale\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        u = torch.bmm(q, k.transpose(1, 2)) # 1.batched matrix multiplication 就是叉乘\n",
    "        u = u / self.scale # 2.Scale\n",
    "\n",
    "        if mask is not None:\n",
    "            u = u.masked_fill(mask, -np.inf) # 3.Mask\n",
    "\n",
    "        attn = self.softmax(u) # 4.Softmax\n",
    "        output = torch.bmm(attn, v) # 5.Output\n",
    "\n",
    "        return attn, output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_q, n_k, n_v = 2, 4, 4\n",
    "    d_q, d_k, d_v = 128, 128, 64\n",
    "    \n",
    "    batch = 32\n",
    "\n",
    "    q = torch.randn(batch, n_q, d_q)\n",
    "    k = torch.randn(batch, n_k, d_k)\n",
    "    v = torch.randn(batch, n_v, d_v)\n",
    "    mask = torch.zeros(batch, n_q, n_k).bool()\n",
    "\n",
    "    attention = ScaledDotProductAttention(scale=np.power(d_k, 0.5))\n",
    "    attn, output = attention(q, k, v, mask=mask)\n",
    "\n",
    "    # print(attn)\n",
    "    # print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multi-Head Attention \"\"\"\n",
    "\n",
    "    def __init__(self, n_head, d_k_, d_v_, d_k, d_v, d_o):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.fc_q = nn.Linear(d_k_, n_head * d_k)\n",
    "        self.fc_k = nn.Linear(d_k_, n_head * d_k)\n",
    "        self.fc_v = nn.Linear(d_v_, n_head * d_v)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(scale=np.power(d_k, 0.5))\n",
    "\n",
    "        self.fc_o = nn.Linear(n_head * d_v, d_o)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        n_head, d_q, d_k, d_v = self.n_head, self.d_k, self.d_k, self.d_v\n",
    "\n",
    "        batch, n_q, d_q_ = q.size()\n",
    "        batch, n_k, d_k_ = k.size()\n",
    "        batch, n_v, d_v_ = v.size()\n",
    "\n",
    "        q = self.fc_q(q) # 1.单头变多头\n",
    "        k = self.fc_k(k)\n",
    "        v = self.fc_v(v)\n",
    "\n",
    "        q = q.view(batch, n_q, n_head, d_q).permute(2, 0, 1, 3).contiguous().view(-1, n_q, d_q)\n",
    "        k = k.view(batch, n_k, n_head, d_k).permute(2, 0, 1, 3).contiguous().view(-1, n_k, d_k)\n",
    "        v = v.view(batch, n_v, n_head, d_v).permute(2, 0, 1, 3).contiguous().view(-1, n_v, d_v)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.repeat(n_head, 1, 1)\n",
    "        attn, output = self.attention(q, k, v, mask=mask) # 2.当成单头注意力求输出\n",
    "\n",
    "        output = output.view(n_head, batch, n_q, d_v).permute(1, 2, 0, 3).contiguous().view(batch, n_q, -1) # 3.Concat\n",
    "        output = self.fc_o(output) # 4.仿射变换得到最终输出\n",
    "\n",
    "        return attn, output\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_q, n_k, n_v = 2, 4, 4\n",
    "    d_q_, d_k_, d_v_ = 128, 128, 64\n",
    "\n",
    "    q = torch.randn(batch, n_q, d_q_)\n",
    "    k = torch.randn(batch, n_k, d_k_)\n",
    "    v = torch.randn(batch, n_v, d_v_)\n",
    "    mask = torch.zeros(batch, n_q, n_k).bool()\n",
    "\n",
    "    mha = MultiHeadAttention(n_head=8, d_k_=128, d_v_=64, d_k=256, d_v=128, d_o=128)\n",
    "    attn, output = mha(q, k, v, mask=mask)\n",
    "\n",
    "    # print(attn.size())\n",
    "    # print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 我自己写了一个self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "class self_attention(nn.Module):\n",
    "    def __init__(self, num_head, n_q, n_k, n_v, d_q, d_k, d_v):\n",
    "        super().__init__()\n",
    "        self.num_head, self.n_q, self.n_k, self.n_v, self.d_q, self.d_k, self.d_v = num_head, n_q, n_k, n_v, d_q, d_k, d_v\n",
    "\n",
    "        self.FC_q = nn.Linear(self.n_q, num_head * self.n_q)\n",
    "        self.FC_k = nn.Linear(self.n_k, num_head * self.n_k)\n",
    "        self.FC_v = nn.Linear(self.n_v, num_head * self.n_v)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "    \n",
    "    def forward(self, q, k, v):\n",
    "        qk = torch.bmm(q, k.transpose(1,2))\n",
    "        qk /= np.sqrt(self.d_k)\n",
    "        qk = self.softmax(qk)\n",
    "        output = torch.bmm(qk, v)\n",
    "\n",
    "        return output\n",
    "        \n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_q, n_k, n_v = 100, 200, 200\n",
    "    d_q, d_k, d_v = 128, 128, 128\n",
    "    num_head = 6\n",
    "\n",
    "    batch_size = 32\n",
    "\n",
    "    atten = self_attention(num_head, n_q, n_k, n_v, d_q, d_k, d_v)\n",
    "\n",
    "    q = torch.randn(batch_size, n_q, d_q)\n",
    "    k = torch.randn(batch_size, n_k, d_k)\n",
    "    v = torch.randn(batch_size, n_v, d_v)\n",
    "\n",
    "    output = atten(q, k, v)\n",
    "\n",
    "    print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maptr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
